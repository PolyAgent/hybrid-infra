{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pyenv/versions/3.10.14/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Requirement already satisfied: python-dotenv in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from -r /content/requirments.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: keras-nlp in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from -r /content/requirments.txt (line 2)) (0.14.0)\n",
      "Requirement already satisfied: keras in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from -r /content/requirments.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: jax in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from -r /content/requirments.txt (line 4)) (0.4.28)\n",
      "Requirement already satisfied: tensorflow-hub in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from -r /content/requirments.txt (line 5)) (0.16.1)\n",
      "Requirement already satisfied: tensorflow in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from -r /content/requirments.txt (line 6)) (2.16.1)\n",
      "Requirement already satisfied: absl-py in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from keras-nlp->-r /content/requirments.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: numpy in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from keras-nlp->-r /content/requirments.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: packaging in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from keras-nlp->-r /content/requirments.txt (line 2)) (24.0)\n",
      "Requirement already satisfied: regex in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from keras-nlp->-r /content/requirments.txt (line 2)) (2024.5.15)\n",
      "Requirement already satisfied: rich in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from keras-nlp->-r /content/requirments.txt (line 2)) (13.7.1)\n",
      "Requirement already satisfied: kagglehub in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from keras-nlp->-r /content/requirments.txt (line 2)) (0.2.5)\n",
      "Requirement already satisfied: tensorflow-text in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from keras-nlp->-r /content/requirments.txt (line 2)) (2.16.1)\n",
      "Requirement already satisfied: namex in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from keras->-r /content/requirments.txt (line 3)) (0.0.8)\n",
      "Requirement already satisfied: h5py in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from keras->-r /content/requirments.txt (line 3)) (3.11.0)\n",
      "Requirement already satisfied: optree in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from keras->-r /content/requirments.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from keras->-r /content/requirments.txt (line 3)) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from jax->-r /content/requirments.txt (line 4)) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from jax->-r /content/requirments.txt (line 4)) (1.13.0)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow-hub->-r /content/requirments.txt (line 5)) (3.20.3)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow-hub->-r /content/requirments.txt (line 5)) (2.16.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow->-r /content/requirments.txt (line 6)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow->-r /content/requirments.txt (line 6)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow->-r /content/requirments.txt (line 6)) (0.5.5)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow->-r /content/requirments.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow->-r /content/requirments.txt (line 6)) (18.1.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow->-r /content/requirments.txt (line 6)) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow->-r /content/requirments.txt (line 6)) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow->-r /content/requirments.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow->-r /content/requirments.txt (line 6)) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow->-r /content/requirments.txt (line 6)) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow->-r /content/requirments.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow->-r /content/requirments.txt (line 6)) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow->-r /content/requirments.txt (line 6)) (2.16.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorflow->-r /content/requirments.txt (line 6)) (0.37.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow->-r /content/requirments.txt (line 6)) (0.43.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow->-r /content/requirments.txt (line 6)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow->-r /content/requirments.txt (line 6)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow->-r /content/requirments.txt (line 6)) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow->-r /content/requirments.txt (line 6)) (2024.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow->-r /content/requirments.txt (line 6)) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow->-r /content/requirments.txt (line 6)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow->-r /content/requirments.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: tqdm in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from kagglehub->keras-nlp->-r /content/requirments.txt (line 2)) (4.66.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from rich->keras-nlp->-r /content/requirments.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from rich->keras-nlp->-r /content/requirments.txt (line 2)) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-nlp->-r /content/requirments.txt (line 2)) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /pyenv/versions/3.10.14/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow->-r /content/requirments.txt (line 6)) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%cd /content\n",
    "%pip install -r /content/requirments.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://hooks.zapier.com/hooks/catch/6996241/23u9vlk/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 02:47:16.426853: E external/xla/xla/stream_executor/rocm/rocm_driver.cc:301] failed call to hipInit: HIP_ERROR_InvalidDevice\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup environment\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint as pp\n",
    "\n",
    "load_dotenv()\n",
    "print(os.environ['ZAPIER_WEBHOOK'])\n",
    "\n",
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 02:47:23.751797: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_7b_en/2/download/model.safetensors...\n",
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_7b_en/2/download/model.safetensors.index.json...\n",
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_7b_en/2/download/task.json...\n",
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_7b_en/2/download/model.safetensors...\n",
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_7b_en/2/download/model.safetensors.index.json...\n",
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_7b_en/2/download/model.safetensors...\n",
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_7b_en/2/download/model.safetensors.index.json...\n",
      "Downloading from https://www.kaggle.com/api/v1/models/keras/gemma/keras/gemma_7b_en/2/download/preprocessor.json...\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "import keras_nlp\n",
    "# keras.config.set_floatx(\"float16\")\n",
    "# Model\n",
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_7b_en\")\n",
    "gemma_lm.preprocessor.tokenizer = keras_nlp.models.GemmaTokenizer(\n",
    "    proto=\"/tokenizer/gemma_ua_ordered.model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts_dev = [\n",
    "    \"\"\"Борщ це -\"\"\",\n",
    "    \"\"\"Крим - це територія\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input_prompts.txt', 'r') as file:\n",
    "    test_prompts = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "def process_prompts():\n",
    "    llm_outputs = []\n",
    "    result_str = \"\"\n",
    "    for prompt in test_prompts:\n",
    "        result_str += f\"PROMPT:\\\"{prompt}\\\"\\n\"\n",
    "        output = gemma_lm.generate(prompt, max_length=96)\n",
    "        llm_outputs.append(output)\n",
    "        result_str += f\"LLM:\\n{output}\\n\\n\"\n",
    "    return result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapier snippet\n",
    "import requests\n",
    "from datetime import datetime\n",
    "webhook_url = os.environ[\"ZAPIER_WEBHOOK\"]\n",
    "model_metadata = \"blank metadata\"\n",
    "body = \"blank body\"\n",
    "def post_to_slack():\n",
    "    requests.post(webhook_url, json={'time': str(datetime.now().isoformat()), 'metadata': model_metadata, 'body': body, 'server': 'AMD mi300x'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata = f\"gemma-7b base model vanilla\"\n",
    "body = process_prompts()\n",
    "post_to_slack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_7b_en\")\n",
    "gemma_lm.preprocessor.tokenizer = keras_nlp.models.GemmaTokenizer(\n",
    "    proto=\"/tokenizer/gemma_ua_ordered.model\"\n",
    ")\n",
    "model_metadata = f\"gemma-7b base model with custom UA tokenizer\"\n",
    "body = process_prompts()\n",
    "post_to_slack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_lm.load_weights(\"/ckpts/ckpt_meaninit_full_step_260000_loss_0.99.weights.h5\")\n",
    "model_metadata = f\"gemma-7b base model with custom UA tokenizer and embd trained on 1B wiki-ua dataset\"\n",
    "body = process_prompts()\n",
    "post_to_slack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_filenames_alphanumerically(filenames):\n",
    "    # Define a function to extract the step value from the filename\n",
    "    def extract_step(filename):\n",
    "        parts = filename.split('_')\n",
    "        step_index = parts.index('step') + 1\n",
    "        return int(parts[step_index])\n",
    "    \n",
    "    # Sort the list using the step value as the key\n",
    "    filenames.sort(key=extract_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_step(filename):\n",
    "    parts = filename.split('_')\n",
    "    step_index = parts.index('step') + 1\n",
    "    return int(parts[step_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ckpt_step_30000_loss_0.923_accuracy_0.137.weights.h5', 'ckpt_step_38000_loss_0.878_accuracy_0.142.weights.h5']\n",
      "30000 ckpt_step_30000_loss_0.923_accuracy_0.137.weights.h5 /tmp/output/EN_UA_2k_mixed_precision_UA_tokenizerckpt_step_30000_loss_0.923_accuracy_0.137.weights.h5\n",
      "38000 ckpt_step_38000_loss_0.878_accuracy_0.142.weights.h5 /tmp/output/EN_UA_2k_mixed_precision_UA_tokenizerckpt_step_38000_loss_0.878_accuracy_0.142.weights.h5\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"/tmp/output/EN_UA_2k_mixed_precision_UA_tokenizer\"\n",
    "file_names = os.listdir(folder_path)\n",
    "sort_filenames_alphanumerically(file_names)\n",
    "print(file_names)\n",
    "percents = [35, 50, 70, 85, 100, 130, 150, 200]\n",
    "for elem in zip(percents, file_names):\n",
    "    iter = extract_step(elem[1])\n",
    "    print(iter, f\"{elem[1]}\", f\"{folder_path}{elem[1]}\")\n",
    "    \n",
    "    # gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_7b_en\")\n",
    "    gemma_lm.load_weights(f\"{folder_path}/{elem[1]}\")\n",
    "    # gemma_lm.preprocessor.tokenizer = keras_nlp.models.GemmaTokenizer(\n",
    "    #     proto=\"/tokenizer/gemma_ua_ordered.model\"\n",
    "    # )\n",
    "    model_metadata = f\"ckpt iter {iter}/132000 EN_UA dataset, custom UA tokenizer mixed precision\"\n",
    "    body = process_prompts()\n",
    "    post_to_slack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ckpt_step_26000_loss_0.600_accuracy_0.172.weights.h5', 'ckpt_step_32000_loss_0.595_accuracy_0.172.weights.h5']\n",
      "26000 ckpt_step_26000_loss_0.600_accuracy_0.172.weights.h5 /tmp/output/EN_UA_2k_mixed_precision_UA_tokenizer_embdckpt_step_26000_loss_0.600_accuracy_0.172.weights.h5\n",
      "32000 ckpt_step_32000_loss_0.595_accuracy_0.172.weights.h5 /tmp/output/EN_UA_2k_mixed_precision_UA_tokenizer_embdckpt_step_32000_loss_0.595_accuracy_0.172.weights.h5\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"/tmp/output/EN_UA_2k_mixed_precision_UA_tokenizer_embd\"\n",
    "file_names = os.listdir(folder_path)\n",
    "sort_filenames_alphanumerically(file_names)\n",
    "print(file_names)\n",
    "percents = [35, 50, 70, 85, 100, 130, 150, 200]\n",
    "for elem in zip(percents, file_names):\n",
    "    iter = extract_step(elem[1])\n",
    "    print(iter, f\"{elem[1]}\", f\"{folder_path}{elem[1]}\")\n",
    "    \n",
    "    # gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_7b_en\")\n",
    "    gemma_lm.load_weights(f\"{folder_path}/{elem[1]}\")\n",
    "    # gemma_lm.preprocessor.tokenizer = keras_nlp.models.GemmaTokenizer(\n",
    "    #     proto=\"/tokenizer/gemma_ua_ordered.model\"\n",
    "    # )\n",
    "    model_metadata = f\"ckpt iter {iter}/132000 EN_UA dataset, custom UA tokenizer mixed precision, embd layer pretraining\"\n",
    "    body = process_prompts()\n",
    "    post_to_slack()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
