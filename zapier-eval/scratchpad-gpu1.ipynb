{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "# %pip install -r /content/requirments.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint as pp\n",
    "\n",
    "load_dotenv()\n",
    "print(os.environ['ZAPIER_WEBHOOK'])\n",
    "\n",
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp\n",
    "# keras.config.set_floatx(\"float16\")\n",
    "# Model\n",
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_7b_en\")\n",
    "gemma_lm.preprocessor.tokenizer = keras_nlp.models.GemmaTokenizer(\n",
    "    proto=\"/tokenizer/gemma_ua_ordered.model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prompt_examples.txt', 'r') as file:\n",
    "    test_prompts = [line.rstrip('\\n') for line in file]\n",
    "    \n",
    "with open('prompt_examples_en.txt', 'r') as file:\n",
    "    test_prompts_en = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "def process_prompts(test_prompts):\n",
    "    llm_outputs = []\n",
    "    for prompt in test_prompts:\n",
    "        output = gemma_lm.generate(prompt, max_length=96)\n",
    "        llm_outputs.append(output)\n",
    "    return llm_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_to_json(results, output_file_path):\n",
    "    with open(output_file_path, \"w\") as outfile:\n",
    "        for result in results:\n",
    "            json_record = json.dumps(result)\n",
    "            outfile.write(f\"{json_record}\\n\")\n",
    "\n",
    "    print(f\"Results saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_filenames_alphanumerically(filenames):\n",
    "    # Define a function to extract the step value from the filename\n",
    "    def extract_step(filename):\n",
    "        parts = filename.split('_')\n",
    "        step_index = parts.index('step') + 1\n",
    "        return int(parts[step_index])\n",
    "    \n",
    "    # Sort the list using the step value as the key\n",
    "    filenames.sort(key=extract_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/output0/EN_UA_2k_mixed_precision_UA_tokenizer/\"\n",
    "file_names = os.listdir(folder_path)\n",
    "sort_filenames_alphanumerically(file_names)\n",
    "results = []\n",
    "for file in file_names:\n",
    "    parts = file[:-11].split('_')\n",
    "    loss = float(parts[parts.index('loss') + 1])\n",
    "    iter = int(parts[parts.index('step') + 1])\n",
    "    accuracy = float(parts[parts.index('accuracy') + 1])\n",
    "    ckpt_path = f\"{folder_path}{file}\"\n",
    "    print(ckpt_path)\n",
    "    gemma_lm.load_weights(ckpt_path)\n",
    "    print(\"load done\")\n",
    "    prompt_results = process_prompts(test_prompts)\n",
    "    prompt_results_en = process_prompts(test_prompts_en)\n",
    "    print(\"prompting done\")\n",
    "    result = {\n",
    "        \"file_name\": file,\n",
    "        \"loss\": loss,\n",
    "        \"iter\": iter,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"prompt_results\": prompt_results,\n",
    "        \"prompt_results_en\": prompt_results_en\n",
    "    }\n",
    "    save_to_json([result], f\"EN_UA_2k_mixed_precision_UA_tokenizer/{file[:-11]}.json\")\n",
    "    results.append(result)\n",
    "save_to_json(results, f\"EN_UA_2k_mixed_precision_UA_tokenizer_embd/results.jsonl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/output1/EN_UA_2k_mixed_precision_UA_tokenizer_embd/\"\n",
    "file_names = os.listdir(folder_path)\n",
    "sort_filenames_alphanumerically(file_names)\n",
    "results = []\n",
    "for file in file_names:\n",
    "    parts = file[:-11].split('_')\n",
    "    loss = float(parts[parts.index('loss') + 1])\n",
    "    iter = int(parts[parts.index('step') + 1])\n",
    "    accuracy = float(parts[parts.index('accuracy') + 1])\n",
    "    ckpt_path = f\"{folder_path}{file}\"\n",
    "    print(ckpt_path)\n",
    "    gemma_lm.load_weights(ckpt_path)\n",
    "    print(\"load done\")\n",
    "    prompt_results = process_prompts(test_prompts)\n",
    "    print(\"prompting done\")\n",
    "    result = {\n",
    "        \"file_name\": file,\n",
    "        \"loss\": loss,\n",
    "        \"iter\": iter,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"prompt_results\": prompt_results,\n",
    "        \"prompt_results_en\": prompt_results_en\n",
    "    }\n",
    "    save_to_json([result], f\"EN_UA_2k_mixed_precision_UA_tokenizer_embd/{file[:-11]}.json\")\n",
    "    results.append(result)\n",
    "save_to_json(results, f\"EN_UA_2k_mixed_precision_UA_tokenizer_embd/results.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
